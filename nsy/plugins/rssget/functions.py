import requests
import json
import feedparser
import httpx
from datetime import datetime
import time
from bs4 import BeautifulSoup
from nonebot import on_command, get_bot, require, Bot
from nonebot.adapters.onebot.v11 import MessageSegment, Message
from nonebot.log import logger
from nonebot_plugin_orm import get_session
from sqlalchemy.exc import SQLAlchemyError


from .models_method import DetailManger



sheet1 = ["aibaaiai","aimi_sound","kudoharuka910","Sae_Otsuka","aoki__hina","Yuki_Nakashim","ttisrn_0710","tanda_hazuki",
          "bang_dream_info","sasakirico","Hina_Youmiya","Riko_kohara","okada_mei0519","AkaneY_banu","Kanon_Takao",
          "Kanon_Shizaki","bushi_creative","amane_bushi","hitaka_mashiro","kohinatamika","AyAsA_violin","romance847",
          "yurishiibot","sakuragawa_megu"]


# é…ç½®é¡¹ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
RSSHUB_HOST = "http://192.168.1.1:1200"  # RSSHub å®ä¾‹åœ°å€
TIMEOUT = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´
MAX_IMAGES = 10  # æœ€å¤šå‘é€å›¾ç‰‡æ•°é‡
API_KEY = "oW4gFumamC9b6gx2ujAKsO1I"
SECRET_KEY = "5HB8M0ik4F2sP35iQVSp7W9fPpAH7dUA"


def extract_content(entry) -> dict:
    """æå–æ¨æ–‡å†…å®¹ç»“æ„åŒ–æ•°æ®"""
    B = BaiDu()
    published = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M")

    # æ¸…ç†æ–‡æœ¬å†…å®¹
    clean_text = BeautifulSoup(entry.description, "html.parser").get_text("\n").strip()
    trans_text = B.main(BeautifulSoup(entry.description, "html.parser").get_text(" "))

    # æå–å›¾ç‰‡ï¼ˆä¼˜å…ˆåª’ä½“å†…å®¹ï¼‰
    images = []
    for media in getattr(entry, "media_content", []):
        if media.get("type", "").startswith("image/"):
            images.append(media["url"])

    # å¦‚æœåª’ä½“å†…å®¹ä¸ºç©ºï¼Œå°è¯•ä»é™„ä»¶è·å–
    if not images:
        for enc in getattr(entry, "enclosures", []):
            if enc.get("type", "").startswith("image/"):
                images.append(enc.href)

    if hasattr(entry, 'description'):
        soup = BeautifulSoup(entry.description, 'html.parser')
        for img in soup.find_all('img', src=True):
            images.append(img['src'])

    return {
        "title": entry.title,
        "time": published,
        "link": entry.link,
        "text": clean_text,
        "trans_title": B.main(entry.title),
        "trans_text": trans_text,
        "images": images[:MAX_IMAGES]
    }

async def fetch_feed(url: str) -> dict:
    """å¼‚æ­¥è·å–å¹¶è§£æRSSå†…å®¹"""
    try:
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            time.sleep(5)
            resp = await client.get(url)
            resp.raise_for_status()
            return feedparser.parse(resp.content)
    except Exception as e:
        logger.error(f"RSSè¯·æ±‚å¤±è´¥: {str(e)}")
        return {"error": f"è·å–å†…å®¹å¤±è´¥: {str(e)}"}



class BaiDu():
    def main(self, body=str):
        url = "https://aip.baidubce.com/rpc/2.0/mt/texttrans/v1?access_token=" + self.get_access_token()

        payload = json.dumps({
            "from": "jp",
            "to": "zh",
            "q": f"{body}"
        }, ensure_ascii=False)
        headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json'
        }

        response = requests.request("POST", url, headers=headers, data=payload.encode("utf-8"))

        json_result = response.text
        result = json.loads(json_result)

        # æå–å•ä¸ª dst
        first_translation = result["result"]["trans_result"][0]["dst"]

        return first_translation


    def get_access_token(self):
        """
        ä½¿ç”¨ AKï¼ŒSK ç”Ÿæˆé‰´æƒç­¾åï¼ˆAccess Tokenï¼‰
        :return: access_tokenï¼Œæˆ–æ˜¯None(å¦‚æœé”™è¯¯)
        """
        url = "https://aip.baidubce.com/oauth/2.0/token"
        params = {"grant_type": "client_credentials", "client_id": API_KEY, "client_secret": SECRET_KEY}
        return str(requests.post(url, params=params).json().get("access_token"))



class rss_get():
    async def send_onebot_image(self,img_url: str, group_id):
        """OneBot ä¸“ç”¨å›¾ç‰‡å‘é€æ–¹æ³•"""
        bot = get_bot()
        try:
            async with httpx.AsyncClient(timeout=20) as client:
                # ä¸‹è½½å›¾ç‰‡æ•°æ®
                resp = await client.get(img_url)
                resp.raise_for_status()

                # æ„é€ å›¾ç‰‡æ¶ˆæ¯æ®µ
                image_seg = MessageSegment.image(resp.content)

                # å‘é€å›¾ç‰‡
                await bot.call_api("send_group_msg", **{
                    "group_id": group_id,
                    "message": image_seg
                })

        except httpx.HTTPError as e:
            logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)}")
            await bot.call_api("send_group_msg", **{
                "group_id": group_id,
                "message": f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼š{e}"
            })
        except Exception as e:
            logger.error(f"å›¾ç‰‡å‘é€å¤±è´¥: {str(e)}")
            await bot.call_api("send_group_msg", **{
                "group_id": group_id,
                "message": f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼š{e}"
            })

    async def handle_rss(self,username: str, group_id: int):
        """å¤„ç†RSSæ¨é€"""
        async with (get_session() as db_session):
            bot = get_bot()
            if username in sheet1:
                feed_url = f"{RSSHUB_HOST}/twitter/user/{username}"
                # è·å–æ•°æ®
                data = await fetch_feed(feed_url)
                # å¤„ç†æœ€æ–°ä¸€æ¡æ¨æ–‡
                latest = data.entries[0]
                published = datetime(*latest.published_parsed[:6]).strftime("%Y-%m-%d %H:%M")
                trueid = published + str(group_id)
                try:
                    # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥ Student_id çš„è®°å½•
                    existing_lanmsg = await DetailManger.get_Sign_by_student_id(
                        db_session, trueid)
                    if existing_lanmsg:  # æ›´æ–°è®°å½•
                        logger.info(f"{published}å·²å­˜åœ¨")
                    else:
                        content = extract_content(latest)
                        try:
                            # å†™å…¥æ•°æ®åº“
                            await DetailManger.create_signmsg(
                                db_session,
                                id=trueid,
                                summary=content['text'],
                            )
                            logger.info(f"åˆ›å»ºæ•°æ®: {content.get('time')}")
                            # æ„å»ºæ–‡å­—æ¶ˆæ¯
                            msg = [
                                f"ğŸ¦ ç”¨æˆ· {username} æœ€æ–°åŠ¨æ€",
                                f"ğŸ“Œ {content['title']}",
                                f"â° {content['time']}",
                                f"ğŸ”— {content['link']}",
                                "\nğŸ“ æ­£æ–‡ï¼š",
                                content['text'],
                                f"ğŸ“Œ {content['trans_title']}"
                                "\nğŸ“ ç¿»è¯‘ï¼š",
                                content["trans_text"],
                            ]

                            # å…ˆå‘é€æ–‡å­—å†…å®¹
                            await bot.call_api("send_group_msg", **{
                                "group_id": group_id,
                                "message": "\n".join(msg)
                            })

                            # å‘é€å›¾ç‰‡ï¼ˆå•ç‹¬å¤„ç†ï¼‰
                            if content["images"]:
                                await bot.call_api("send_group_msg", **{
                                    "group_id": group_id,
                                    "message": f"ğŸ–¼ï¸ æ£€æµ‹åˆ° {len(content['images'])} å¼ å›¾ç‰‡..."
                                })
                                for index, img_url in enumerate(content["images"], 1):
                                    await rss_get.send_onebot_image(self, img_url, group_id)
                        except Exception as e:
                            logger.error(f"å¤„ç† {content.get('time')} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                except SQLAlchemyError as e:
                    logger.error(f"æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")