import feedparser
import httpx
from datetime import datetime
import time
from apscheduler.triggers.cron import CronTrigger
from bs4 import BeautifulSoup
from nonebot import on_command, get_bot, require, Bot
from nonebot.adapters.onebot.v11 import MessageSegment, Message
from nonebot.params import CommandArg
from nonebot.plugin import PluginMetadata
from nonebot.log import logger

from .functions import BaiDu, rss_get
from .models_method import DetailManger
from .models import Detail


__plugin_meta__ = PluginMetadata(
    name="Twitter RSSè®¢é˜…",
    description="é€šè¿‡RSSHubè·å–Twitterç”¨æˆ·æœ€æ–°åŠ¨æ€å¹¶å‘é€å›¾ç‰‡",
    usage="/rss [ç”¨æˆ·å]  # è·å–æŒ‡å®šç”¨æˆ·æœ€æ–°æ¨æ–‡",
    type="application",
    homepage="https://github.com/your/repo",
)
B = BaiDu()  # åˆå§‹åŒ–ç¿»è¯‘ç±»
R = rss_get()  # åˆå§‹åŒ–rssç±»
sheet1 = ["aibaaiai","aimi_sound","kudoharuka910","Sae_Otsuka","aoki__hina","Yuki_Nakashim","ttisrn_0710","tanda_hazuki",
          "bang_dream_info","sasakirico","Hina_Youmiya","Riko_kohara","okada_mei0519","AkaneY_banu","Kanon_Takao",
          "Kanon_Shizaki","bushi_creative","amane_bushi","hitaka_mashiro","kohinatamika","AyAsA_violin"]


# é…ç½®é¡¹ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰
RSSHUB_HOST = "https://rsshub.app"  # RSSHub å®ä¾‹åœ°å€
TIMEOUT = 30  # è¯·æ±‚è¶…æ—¶æ—¶é—´
MAX_IMAGES = 10  # æœ€å¤šå‘é€å›¾ç‰‡æ•°é‡

scheduler = require("nonebot_plugin_apscheduler").scheduler

rss_cmd = on_command("rss",priority=10,block=True)



async def fetch_feed(url: str) -> dict:
    """å¼‚æ­¥è·å–å¹¶è§£æRSSå†…å®¹"""
    try:
        async with httpx.AsyncClient(timeout=TIMEOUT) as client:
            resp = await client.get(url)
            resp.raise_for_status()
            return feedparser.parse(resp.content)
    except Exception as e:
        logger.error(f"RSSè¯·æ±‚å¤±è´¥: {str(e)}")
        return {"error": f"è·å–å†…å®¹å¤±è´¥: {str(e)}"}


def extract_content(entry) -> dict:
    """æå–æ¨æ–‡å†…å®¹ç»“æ„åŒ–æ•°æ®"""
    published = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M")

    # æ¸…ç†æ–‡æœ¬å†…å®¹
    clean_text = BeautifulSoup(entry.description, "html.parser").get_text("\n").strip()
    trans_text = B.main(BeautifulSoup(entry.description, "html.parser").get_text(" "))

    # æå–å›¾ç‰‡ï¼ˆä¼˜å…ˆåª’ä½“å†…å®¹ï¼‰
    images = []
    for media in getattr(entry, "media_content", []):
        if media.get("type", "").startswith("image/"):
            images.append(media["url"])

    # å¦‚æœåª’ä½“å†…å®¹ä¸ºç©ºï¼Œå°è¯•ä»é™„ä»¶è·å–
    if not images:
        for enc in getattr(entry, "enclosures", []):
            if enc.get("type", "").startswith("image/"):
                images.append(enc.href)

    if hasattr(entry, 'description'):
        soup = BeautifulSoup(entry.description, 'html.parser')
        for img in soup.find_all('img', src=True):
            images.append(img['src'])

    return {
        "title": entry.title,
        "time": published,
        "link": entry.link,
        "text": clean_text,
        "trans_title": B.main(entry.title),
        "trans_text": trans_text,
        "images": images[:MAX_IMAGES]
    }


async def send_onebot_image(img_url: str):
    """OneBot ä¸“ç”¨å›¾ç‰‡å‘é€æ–¹æ³•"""
    bot = get_bot()
    try:
        async with httpx.AsyncClient(timeout=15) as client:
            # ä¸‹è½½å›¾ç‰‡æ•°æ®
            resp = await client.get(img_url)
            resp.raise_for_status()

            # æ„é€ å›¾ç‰‡æ¶ˆæ¯æ®µ
            image_seg = MessageSegment.image(resp.content)

            # å‘é€å›¾ç‰‡
            await rss_cmd.send(image_seg)

    except httpx.HTTPError as e:
        logger.error(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {str(e)}")
        await rss_cmd.send(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼š{e}")
    except Exception as e:
        logger.error(f"å›¾ç‰‡å‘é€å¤±è´¥: {str(e)}")
        await rss_cmd.send(f"å›¾ç‰‡å‘é€å¤±è´¥ï¼š{e}")


@rss_cmd.handle()
async def handle_rss(args: Message = CommandArg()):
    username = args.extract_plain_text().strip()
    if not username:
        await rss_cmd.finish("è¯·è¾“å…¥Twitterç”¨æˆ·åï¼Œä¾‹å¦‚ï¼š/rss aibaaiai")
    elif username not in sheet1:
        await rss_cmd.finish("è¯·æ±‚è¢«å¦å†³")
    else:
        feed_url = f"{RSSHUB_HOST}/twitter/user/{username}"

        # è·å–æ•°æ®
        data = await fetch_feed(feed_url)
        if "error" in data:
            await rss_cmd.finish(data["error"])

        if not data.get("entries"):
            await rss_cmd.finish("è¯¥ç”¨æˆ·æš‚æ— åŠ¨æ€æˆ–ä¸å­˜åœ¨")

        # å¤„ç†æœ€æ–°ä¸€æ¡æ¨æ–‡
        latest = data.entries[0]
        content = extract_content(latest)

        # æ„å»ºæ–‡å­—æ¶ˆæ¯
        msg = [
            f"ğŸ¦ ç”¨æˆ· {username} æœ€æ–°åŠ¨æ€",
            f"ğŸ“Œ {content['title']}",
            f"â° {content['time']}",
            f"ğŸ”— {content['link']}",
            "\nğŸ“ æ­£æ–‡ï¼š",
            content['text'],
            f"ğŸ“Œ {content['trans_title']}"
            "\nğŸ“ ç¿»è¯‘ï¼š",
            content["trans_text"],
        ]

        # å…ˆå‘é€æ–‡å­—å†…å®¹
        await rss_cmd.send("\n".join(msg))

        # å‘é€å›¾ç‰‡ï¼ˆå•ç‹¬å¤„ç†ï¼‰
        if content["images"]:
            await rss_cmd.send(f"ğŸ–¼ï¸ æ£€æµ‹åˆ° {len(content['images'])} å¼ å›¾ç‰‡...")
            for index, img_url in enumerate(content["images"], 1):
                await send_onebot_image(img_url)


@scheduler.scheduled_job(CronTrigger(minute="*/10"))
async def auto_update_func():
    await R.handle_rss("aibaaiai", 1016925587)
    time.sleep(3)
    await R.handle_rss("bang_dream_info", 1016925587)
    time.sleep(3)
    await R.handle_rss("bang_dream_info", 824993838)
    time.sleep(3)
    await R.handle_rss("kohinatamika", 824993838)
    time.sleep(3)
    await R.handle_rss("AyAsA_violin", 824993838)
    time.sleep(3)
    await R.handle_rss("aimi_sound", 824993838)
