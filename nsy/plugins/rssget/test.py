import feedparser
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from io import BytesIO
from PIL import Image
from rich.console import Console


def get_rss_feed(url):
    """è·å–å¹¶è§£æ RSS å†…å®¹"""
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except Exception as e:
        print(f"è·å–å†…å®¹å¤±è´¥: {e}")
        return None


def get_terminal_size():
    """è·å–ç»ˆç«¯æ˜¾ç¤ºå°ºå¯¸"""
    try:
        from shutil import get_terminal_size as gts
        return gts().columns // 2, gts().lines - 10
    except:
        return (40, 20)  # é»˜è®¤å°ºå¯¸


def show_terminal_image(url, max_size=None):
    """åœ¨ç»ˆç«¯æ˜¾ç¤ºå›¾ç‰‡"""
    try:
        response = requests.get(url, stream=True, timeout=10)
        img = Image.open(BytesIO(response.content))

        # è‡ªåŠ¨ç¼©æ”¾å›¾ç‰‡
        if max_size:
            img.thumbnail((max_size[0] * 10, max_size[1] * 6))  # æ ¹æ®å­—ç¬¦æ¯”ä¾‹è°ƒæ•´

        console = Console()
        with console.capture() as capture:
            console.print(f"[image]{url}")
        return capture.get()
    except Exception as e:
        print(f"å›¾ç‰‡æ˜¾ç¤ºå¤±è´¥: {e}")
        return None


def extract_images(entry):
    """æå–å›¾ç‰‡é“¾æ¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    sources = set()

    # ä»åª’ä½“å†…å®¹æå–
    for media in getattr(entry, 'media_content', []):
        if media.get('type', '').startswith('image/'):
            sources.add(media['url'])

    # ä»é™„ä»¶æå–
    for enc in getattr(entry, 'enclosures', []):
        if enc.get('type', '').startswith('image/'):
            sources.add(enc.href)

    # ä»HTMLæè¿°æå–
    if hasattr(entry, 'description'):
        soup = BeautifulSoup(entry.description, 'html.parser')
        for img in soup.find_all('img', src=True):
            sources.add(img['src'])

    return list(sources)


def display_feed(feed):
    """å¸¦å›¾ç‰‡å±•ç¤ºçš„å†…å®¹æ˜¾ç¤º"""
    if not feed or not feed.entries:
        print("æ²¡æœ‰è·å–åˆ°å†…å®¹")
        return

    console = Console()
    console.print(f"\n[bold cyan]=== {feed.feed.title} ===", justify="center")

    if feed.entries:
        entry = feed.entries[0]
        # åŸºæœ¬ä¿¡æ¯
        published = datetime(*entry.published_parsed[:6]).strftime("%Y-%m-%d %H:%M")
        console.print(f"\n[bright_yellow]ğŸ“Œ {entry.title}[/]")
        console.print(f"â° {published}  ğŸ”— [link]{entry.link}")

        # æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
        if hasattr(entry, 'description'):
            clean_text = BeautifulSoup(entry.description, 'html.parser').get_text()
            console.print(f"[dim]{clean_text}[/]")

        # å›¾ç‰‡æ˜¾ç¤º
        images = extract_images(entry)
        if images:
            console.print("\nğŸ–¼ï¸ [bold]å›¾ç‰‡é¢„è§ˆ:[/]")
            term_size = get_terminal_size()
            for url in images:  # æœ€å¤šæ˜¾ç¤º3å¼ å›¾ç‰‡
                console.print(f"  [dim]å›¾ç‰‡åœ°å€: {url}[/]")
                show_terminal_image(url, max_size=term_size)
                console.print("\n")
        else:
            console.print("\n[dim]æ— å›¾ç‰‡å†…å®¹[/]")

        console.print("-" * 40)


if __name__ == "__main__":
    # é…ç½®ä¿¡æ¯
    RSSHUB_INSTANCE = "https://rsshub.app"  # æ¨èä½¿ç”¨è‡ªå»ºå®ä¾‹
    TWITTER_USER = "bang_dream_info"  # æ›¿æ¢ç›®æ ‡ç”¨æˆ·å

    feed_url = f"{RSSHUB_INSTANCE}/twitter/user/{TWITTER_USER}"

    console = Console()
    with console.status("[bold green]æ­£åœ¨è·å–æœ€æ–°åŠ¨æ€..."):
        feed_data = get_rss_feed(feed_url)

    if feed_data:
        display_feed(feed_data)